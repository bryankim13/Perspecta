{
  "metadata": {
    "name": "Heartbeat Modeling",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "from pandas import read_csv, concat\nfrom pandas import Series\nfrom numpy import array\nfrom numpy import arange, asarray\nfrom matplotlib import pyplot\nimport itertools\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import resample"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df \u003d read_csv(\u0027/mnt/data/data/bryan/heartbeat/mitbih_train.csv\u0027, header \u003d None)\ndfTest \u003d read_csv(\u0027/mnt/data/data/bryan/heartbeat/mitbih_test.csv\u0027, header \u003d None)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "distribution \u003d df[187].value_counts()\nname \u003d [\u0027n\u0027,\u0027q\u0027,\u0027v\u0027,\u0027s\u0027,\u0027f\u0027]\npyplot.bar(name,distribution)\npyplot.title(\"HeartBeat Distribution\")\npyplot.ylabel(\"Frequency\")\npyplot.xlabel(\"Classification\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df0 \u003d df[df[187]\u003d\u003d0].sample(20000, random_state \u003d 100)\ndf1 \u003d df[df[187]\u003d\u003d1]\ndf2 \u003d df[df[187]\u003d\u003d2]\ndf3 \u003d df[df[187]\u003d\u003d3]\ndf4 \u003d df[df[187]\u003d\u003d4]\ndf1OS \u003d resample(df1, replace \u003d True, n_samples \u003d 20000, random_state \u003d 110)\ndf2OS \u003d resample(df2, replace \u003d True, n_samples \u003d 20000, random_state \u003d 120)\ndf3OS \u003d resample(df3, replace \u003d True, n_samples \u003d 20000, random_state \u003d 130)\ndf4OS \u003d resample(df4, replace \u003d True, n_samples \u003d 20000, random_state \u003d 140)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfTotal \u003d concat([df0,df1OS,df2OS,df3OS,df4OS])\nfrom pandas import DataFrame\nx \u003d dfTotal.values\ndfTotal \u003d DataFrame(x)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "distribution \u003d dfTotal[187].value_counts()\nname \u003d [\u0027n\u0027,\u0027q\u0027,\u0027v\u0027,\u0027s\u0027,\u0027f\u0027]\npyplot.bar(name,distribution)\npyplot.title(\"Redistributed Heartbeat\")\npyplot.xlabel(\"Classification\")\npyplot.ylabel(\"Frequency\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfTotal\u003ddfTotal.sample(frac \u003d 1)\ndfTotal.shape"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "trainX \u003d dfTotal.iloc[:,dfTotal.columns!\u003d187]\ntrainY \u003d dfTotal.iloc[:,dfTotal.columns\u003d\u003d187]"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfTest\u003ddfTest.sample(frac \u003d 1)\ntestX \u003d dfTest.iloc[:,dfTest.columns !\u003d 187]\ntestY \u003d dfTest.iloc[:,dfTest.columns \u003d\u003d 187]"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "models \u003d list()\nnames \u003d list()\nmodels.append(KNeighborsClassifier())\nnames.append(\"KNN\")\nmodels.append(RandomForestClassifier())\nnames.append(\"RF\")\nallscores \u003d list()\nfor i in range(len(models)):\n    s \u003d StandardScaler()\n    p \u003d Pipeline(steps\u003d[(\u0027s\u0027,s), (\u0027m\u0027 , models[i])])\n    scores \u003d cross_val_score(p, trainX, trainY, scoring\u003d\u0027accuracy\u0027, cv\u003d5, n_jobs\u003d-1)\n    allscores.append(scores)\n    m,s \u003d mean(scores)*100, std(scores)*100\n    print(\u0027%s %.3f%% +/-%.3f\u0027 % (names[i], m, s))\npyplot.boxplot(allscores,labels \u003d names)\npyplot.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pyplot.boxplot(allscores,labels \u003d names)\npyplot.ylabel(\"Accuracy\")\npyplot.xlabel(\"Type of Model\")\npyplot.title(\"Cross Validation Accuracy\")\npyplot.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "# evaluate models\nall_scores \u003d list()\nscaler \u003d StandardScaler()\nmodel \u003d Pipeline(steps\u003d[(\u0027s\u0027,scaler), (\u0027m\u0027,models[1])])\n    \nmodel.fit(trainX, trainY) #Swapped the training and testing, resulted in better f1 score but take note \n#more data trained is usually better\n# predict\nyhat \u003d model.predict(testX)\n# evaluate\nscore \u003d f1_score(testY, yhat, average \u003d \u0027macro\u0027) * 100\nprint(confusion_matrix(testY, yhat))\n    \nall_scores.append(score)\n# summarize\nprint(\u0027%s %.3f%%\u0027 % (names[1], score))\nprint(accuracy_score(testY,yhat)*100)\n# plot\npyplot.bar(names, all_scores)\npyplot.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "from numpy import newaxis\ndef plot_confusion_matrix(cm, classes,\n                        normalize\u003dFalse,\n                        title\u003d\u0027Confusion matrix\u0027,\n                        cmap\u003dplt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize\u003dTrue`.\n    \"\"\"\n    plt.imshow(cm, interpolation\u003d\u0027nearest\u0027, cmap\u003dcmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks \u003d arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation\u003d45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm \u003d cm.astype(\u0027float\u0027) / cm.sum(axis\u003d1)[:, newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print(\u0027Confusion matrix, without normalization\u0027)\n\n    print(cm)\n\n    thresh \u003d cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment\u003d\"center\",\n            color\u003d\"white\" if cm[i, j] \u003e thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel(\u0027True label\u0027)\n    plt.xlabel(\u0027Predicted label\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "plot_confusion_matrix(cm\u003dconfusion_matrix(testY, yhat),classes\u003d[\u0027N\u0027, \u0027S\u0027, \u0027V\u0027, \u0027F\u0027, \u0027Q\u0027], title \u003d \"Confusion Matrix\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import joblib\njoblib.dump(model, \u0027heartbeatModel.pkl\u0027)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "# evaluate models\nall_scores \u003d list()\nfor i in range(len(models)):\n\t# create a pipeline for the model\n\tscaler \u003d StandardScaler()\n\tmodel \u003d Pipeline(steps\u003d[(\u0027s\u0027,scaler), (\u0027m\u0027,models[i])])\n\t# fit\n\t# model \u003d models[i]\n# \tmodel.fit(sequencesTrain, targetTrain)\n# \t# predict\n# \tyhat \u003d model.predict(sequences1)\n# \t# evaluate\n# \tscore \u003d f1_score(target1, yhat, average \u003d \u0027macro\u0027) * 100\n    #score \u003d accuracy_score(target1,yhat)*100\n    \n\tmodel.fit(trainX, trainY) #Swapped the training and testing, resulted in better f1 score but take note \n    #more data trained is usually better\n\t# predict\n\tyhat \u003d model.predict(testX)\n\t# evaluate\n\tscore \u003d f1_score(testY, yhat, average \u003d \u0027macro\u0027) * 100\n\tscore2 \u003d accuracy_score(testY,yhat)*100\n\t\n\t#print(confusion_matrix(testY, yhat))\n\n    \n\tall_scores.append(score)\n\t# summarize\n\tprint(\u0027%s F1 Score: %.3f%% Accuracy: %.3f$$\u0027 % (names[i], score,score2))\n# plot\npyplot.bar(names, all_scores)\npyplot.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pyplot.plot(df0.iloc[1])"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pyplot.plot(df1.iloc[1])"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pyplot.plot(df2.iloc[2])"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pyplot.plot(df3.iloc[2])"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pyplot.plot(df4.iloc[22])"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "all_scores \u003d [92.94,97.657,91.23]\npyplot.bar(names, all_scores)\npyplot.title(\"Accuracy of Models\")\npyplot.xlabel(\"Model\")\npyplot.ylabel(\"Accuracy\")\npyplot.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfTotal \u003d concat([df0,df1OS,df2OS,df3OS,df4OS])\nfrom pandas import DataFrame\nx \u003d dfTotal.values\ndfTotal \u003d DataFrame(x)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfx \u003d dfTotal.iloc[:,dfTotal.columns!\u003d187]\ndfy \u003d dfTotal.iloc[:,dfTotal.columns\u003d\u003d187]\ndfx \u003d dfx.diff(periods\u003d1, axis \u003d 1)\ndfx \u003d dfx.iloc[:,dfx.columns!\u003d0]\ndfx[187] \u003d asarray(dfy)\nx \u003d dfx.values\ndfx \u003d DataFrame(x)\nprint(dfx)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dft \u003d dfTest.iloc[:,dfTest.columns!\u003d187]\ndfy \u003d dfTest.iloc[:,dfTest.columns\u003d\u003d187]\ndft \u003d dft.diff(periods\u003d1, axis \u003d 1)\ndft \u003d dft.iloc[:,dft.columns!\u003d0]\ndft[187] \u003d asarray(dfy)\nx \u003d dft.values\ndft \u003d DataFrame(x)\nprint(dft)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfx\u003ddfx.sample(frac \u003d 1)\ntrainX \u003d dfx.iloc[:,dfx.columns!\u003d186]\ntrainY \u003d dfx.iloc[:,dfx.columns\u003d\u003d186]\ndft\u003ddft.sample(frac \u003d 1)\ntestX \u003d dft.iloc[:,dft.columns !\u003d 186]\ntestY \u003d dft.iloc[:,dft.columns \u003d\u003d 186]"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "# evaluate models\nall_scores \u003d list()\nfor i in range(len(models)):\n\t# create a pipeline for the model\n\tscaler \u003d StandardScaler()\n\tmodel \u003d Pipeline(steps\u003d[(\u0027s\u0027,scaler), (\u0027m\u0027,models[i])])\n\t# fit\n\t# model \u003d models[i]\n# \tmodel.fit(sequencesTrain, targetTrain)\n# \t# predict\n# \tyhat \u003d model.predict(sequences1)\n# \t# evaluate\n# \tscore \u003d f1_score(target1, yhat, average \u003d \u0027macro\u0027) * 100\n    #score \u003d accuracy_score(target1,yhat)*100\n    \n\tmodel.fit(trainX, trainY) #Swapped the training and testing, resulted in better f1 score but take note \n    #more data trained is usually better\n\t# predict\n\tyhat \u003d model.predict(testX)\n\t# evaluate\n\tscore \u003d f1_score(testY, yhat, average \u003d \u0027macro\u0027) * 100\n\tprint(confusion_matrix(testY, yhat))\n\n    \n\tall_scores.append(score)\n\t# summarize\n\tprint(\u0027%s %.3f%%\u0027 % (names[i], score))\n# plot\npyplot.bar(names, all_scores)\npyplot.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "m \u003d joblib.load(\u0027heartbeatModel.pkl\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "nn \u003d df.iloc[1][:187]\nnn \u003d [nn]"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "model.predict(nn)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import zipfile\nzipfile.ZipFile(\u0027hbmodel.zip\u0027, mode\u003d\u0027w\u0027).write(\"heartbeatModel.pkl\")"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "type(model)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import sklearn\r\nprint(sklearn.__version__)\r\n"
    }
  ]
}